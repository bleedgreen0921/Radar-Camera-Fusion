# 雷达-相机融合算法

## CenterFusion: Center-based Radar and Camera Fusion for 3D Object Detection

本文主要研究雷达和相机传感器融合问题，提出了一种**中间融合（middle-fusion）**方法，利用雷达和相机数据进行3D目标检测。我们的方法称为**CenterFusion**，首先使用**中心点检测网络**通过识别图像上的中心点来检测对象。然后，它使用一种新的**基于平截头体的方法**将雷达检测与相应对象的中心点相关联，从而解决了关键数据关联问题。相关的雷达检测用于生成基于雷达的特征图，以补充图像特征，最后利用融合后的特征来精确地估计目标的三维属性，如深度、旋转和速度等。

在早期融合方法中，来自不同模态传感器的原始或预处理的数据被融合在一起。通过这种方法，网络从传感模态中学习联合表示。早期的融合方法通常对数据的空间或时间错位很敏感。另一方面，后期融合方法在决策层面结合了来自不同模态的数据，并为向网络引入新的传感模态提供了更大的灵活性。然而，后期融合的方法并没有充分利用现有传感模态的潜力，因为它没有获取通过学习联合表示获得的中间特征。

早期和晚期融合方法之间的折衷称为中期融合。它从不同的模态中单独提取特征，并在中间阶段将其组合起来，使网络能够学习联合表示，并在敏感性和灵活性之间取得平衡。S

------

### CenterFusion

CenterFusion专注于将雷达检测与从图像中获得的初步检测结果相关联，然后生成雷达特征图，并将其与图像特征一起使用，以准确估计物体的3D边界框。特别是，我们使用关键点检测网络生成初步的3D检测，并提出了一种新的基于截锥的雷达关联方法，将雷达检测与3D空间中的相应对象精确关联。然后，这些雷达检测被映射到图像平面，并用于创建特征图，以补充基于图像的特征。最后，融合特征用于精确估计物体的三维属性，如深度、旋转和速度。

融合机制的关键是将雷达探测与物体准确关联。中心点物体检测网络为图像中的每个物体类别生成热图。热图中的峰值代表物体的可能中心点，这些位置的图像特征用于估计其他物体属性。为了利用这种设置中的雷达信息，基于雷达的特征需要被映射到图像上其对应对象的中心，这要求雷达检测与场景中的对象之间的准确关联。

![image-20250705143559856](C:\Users\adminqiu\AppData\Roaming\Typora\typora-user-images\image-20250705143559856.png)

------

#### Center Point Detection（CenterNet）

我们采用CenterNet [34]检测网络对图像进行初步检测。首先使用完全卷积的编码器解码器骨干网络提取图像特征。我们遵循CenterNet并使用**深层聚合（DLA）**网络的修改版本作为骨干。然后使用提取的图像特征来预测图像上的对象中心点，以及物体的2D尺寸（宽度和高度）、中心偏移、3D尺寸、深度和旋转。这些值由主回归头预测，如图1所示。每个主回归头由具有256个通道的3 × 3卷积层和1 × 1卷积层组成，以生成所需的输出。这提供了准确的2D边界框以及场景中每个检测到的对象的初步3D边界框。

------

#### Radar Association

为了充分利用雷达数据，首先需要将雷达检测与图像平面上的相应对象相关联。为了实现这一点，一种简单的方法是将每个雷达探测点映射到图像平面，如果该点映射到物体的2D边界框内，则将其与该物体相关联，这不是一个非常鲁棒的解决方案，因为在雷达探测和图像中的对象之间没有一对一的映射。场景中很多物体会产生多个雷达探测，也有雷达探测不对应任何物体的情况。

此外，由于雷达检测的z维度不准确（或根本不存在），因此映射的雷达检测可能会在其对应对象的2D边界框之外结束。

最后，从被遮挡物体获得的雷达检测将映射到图像中的相同的一般区域，这使得在2D图像平面中区分它们变得困难。

**Frustum Association Mechanism:截头体关联机制**

![image-20250705152334051](C:\Users\adminqiu\AppData\Roaming\Typora\typora-user-images\image-20250705152334051.png)

我们开发了一种平截头体关联方法，该方法使用对象的2D边界框以及其估计的深度和大小来为对象创建3D感兴趣区域（RoI）平截头体。有了对象的精确2D边界框，我们为该对象创建一个平截头体，如图3所示。这大大缩小了需要检查关联的雷达检测，因为在这个截头体之外的任何点都可以忽略。然后我们使用估计的**物体深度，尺寸和旋转**来创建物体周围的RoI，以进一步过滤掉与该物体无关的雷达检测。如果在这个RoI内有多个雷达检测，我们将最近的点作为该物体对应的雷达检测。

在训练阶段，我们使用物体的3D**真实**边界框来创建一个紧密的RoI平截体，并将雷达检测与物体相关联。在测试阶段，RoI平截体使用物体的**估计**3D边界框来计算，如前所述。在这种情况下，我们使用参数δ来控制RoI平截体的大小，如图3所示。

这是为了解释估计深度值的不准确性，因为在这个阶段物体的深度仅使用基于图像的特征来确定。使用该参数扩大截头体增加了将相应的雷达探测包括在截头体内的机会，即使估计深度稍微偏离。δ的值应仔细选择，因为大的ROI截头体可能包括附近物体的雷达检测。

RoI平截头体方法使关联重叠对象变得轻而易举，因为对象在3D空间中是分开的，并且会有单独的RoI平截体。它还消除了多检测关联问题，因为只有RoI平截头体内最近的雷达检测与对象相关联。然而，它不能帮助解决z维度不准确的问题，因为雷达检测可能由于高度信息不准确而位于其相应对象的ROI平截头体之外。

------

#### **Pillar Expansion**

![image-20250705154742103](C:\Users\adminqiu\AppData\Roaming\Typora\typora-user-images\image-20250705154742103.png)

为了解决高度信息不准确的问题，我们引入了一个称为柱扩展的雷达点云预处理步骤，其中每个雷达点扩展为固定大小的柱，如图4所示。柱子为雷达检测到的物理对象创建了更好的表示，因为这些检测现在与3D空间中的维度相关联。有了这个新的表示，如果其对应柱子的全部或部分在截头锥体内，则我们简单地认为雷达检测在截头锥体内，如图1所示。

将雷达点扩展到3D支柱（上图）。

将支柱直接映射到图像并替换为雷达深度信息会导致与物体中心的关联性差，并导致许多重叠的深度值（中间图像）。

Frustum关联准确地将雷达探测映射到物体的中心，并最大限度地减少重叠（底部图像）。

雷达检测仅与具有有效地面实况或检测框的对象相关联，并且仅当雷达探测柱的全部或部分位于框内时。平截头体关联还可以防止将建筑物等背景物体引起的雷达检测与前景物体相关联，正如图像右侧的行人所示。

------

#### Radar Feature Extraction

将雷达检测与相应的物体关联起来后，我们使用雷达检测的深度和速度来为图像创建补充特征。特别是，对于与对象相关的每个雷达检测，我们生成三个以对象的2D边界框为中心并在对象的2D边界框内的热图通道，如图4所示。热图的宽度和高度与对象的2D边界框成比例，并由参数a控制。如果两个对象具有重叠的热图区域，则深度值较小的对象占主导地位，因为只有最近的对象在图像中完全可见。

然后，生成的热图将作为额外通道连接到图像特征。这些特征被用作二次回归头的输入，以重新计算对象的深度和旋转，以及速度和属性。速度回归头估计车辆坐标系中物体实际速度的x和y分量。属性回归头估计不同对象类别的不同属性，例如“汽车”类别的移动或停车以及“步行”类别的站立或坐下。

二次回归头由三个具有3×3核的卷积层组成，后面是1×1卷积层，以生成所需的输出。与主回归头相比，额外的卷积层有助于从雷达特征图中学习更高级的特征

最后一步是将回归头结果解码为3D边界框。框解码器块使用二次回归头估计的深度、速度、旋转和属性，并从主回归头获取其他对象属性。

------

## **Objects as Points-CenterNet**

当前的对象检测器通过一个紧密包围对象的轴对齐边界框表示每个对象。然后，它们将对象检测简化为对大量潜在对象边界框的图像分类。对于每个边界框，分类器确定图像内容是特定对象还是背景。

单阶段探测器在图像上滑动可能的边界框（称为锚点）的复杂排列，并直接对其进行分类，而无需指定框内容。

两阶段探测器重新计算每个潜在框的图像特征，然后对这些特征进行分类。后处理，即非最大值抑制，然后通过计算边界框IoU来消除同一实例的重复检测。这种后处理很难区分和训练，因此大多数当前的检测器都不是端到端可训练的。

Faster-RCNN在检测网络中生成区域建议。它对低分辨率图像网格周围的固定形状边界框（锚点）进行采样，并将每个边界框分类为“前景或非前景”。锚点被标记为与任何地面实况对象具有a>0.7重叠的前景，具有a< 0.3重叠的背景，或者被忽略。每个生成的区域建议再次分类。

在本文中，我们提供了一种更简单、更有效的替代方案。我们通过边界框中心的一个点来表示对象（参见图2）。然后，直接从中心位置的图像特征回归其他属性，例如对象大小、维度、3D范围、方向和姿态。因此，对象检测是一个标准的关键点估计问题。我们只需将输入图像提供给全卷积网络，从而生成热图。该热图中的峰值对应于物体中心。每个峰值处的图像特征预测对象边界框的高度和重量。该模型使用标准密集监督学习进行训练。推理是一个单一的网络前向传递，没有用于后处理的非最大抑制。

![image-20250705170043830](C:\Users\adminqiu\AppData\Roaming\Typora\typora-user-images\image-20250705170043830.png)

我们的方法与基于锚点的单阶段方法密切相关。中心点可以被视为单个形状不可知的锚（见图3）。然而，有一些重要的差异。

1. 我们的CenterNet仅根据位置分配“锚”，而不是框重叠。我们没有用于前景和背景分类的手动阈值。
2. 每个对象只有一个正“锚点”，因此不需要非最大抑制（NMS），我们只是在关键点热图中提取局部峰值
3. 与传统的对象探测器（输出步长为16）相比，CenterNet使用了更大的输出分辨率（输出步长为4），这消除了对多个锚的需要。

![image-20250705171952917](C:\Users\adminqiu\AppData\Roaming\Typora\typora-user-images\image-20250705171952917.png)

------

### 模型准备

**核心目标：** 输入一张图像 `I`，输出一张关键点热图 `Ŷ`，用于检测图像中特定类型关键点的位置（例如人体关节、物体中心等）。

1. **输入和输出定义：**

   - **输入图像 `I`:** 维度为 $W × H × 3$ (宽度 × 高度 × RGB颜色通道)。
   - 预测的热图 `Ŷ`: 维度为 $(W/R) × (H/R) × C$。
     - $W/R$ 和 $H/R$：输出的宽度和高度，是输入宽高的 `1/R`（因为 `R` 是输出步长(output stride)）。
     - `C`：关键点类型的数量（例如，17种人体关节姿态或80种物体类别）。
     - Ŷ的值域：[0, 1]。其中：
       - $Ŷ(x,y,c) = 1$：表示在位置 `(x, y)` 上检测到了类型为 `c` 的关键点。
       - $Ŷ(x,y,c) = 0$：表示位置 `(x, y)` 是背景。
     - **输出步长 `R`:** 是一个下采样因子，决定了输出的预测图相对于输入图像的分辨率降低了多少倍。`R=4` 是文献中常用的默认值（输入512x512时输出128x128）。较大的 `R` 能提升速度、降低内存消耗并增大感受野，但可能会损失定位精度（之后会用**局部偏移**补偿这个精度损失）。

2. **训练目标（真值热图 `Y` 的构造）：**

   - 对于图像中的一个真实关键点 `p`（属于类别 `c`），它位于高分辨率的图像坐标（例如 $[x_p, y_p]$）。计算其在低分辨率热图上的等价位置：$p = floor(p / R)$（向下取整）。
   - 使用高斯核将所有真值关键点映射到热图中，构造真值热图 Y（维度与 Ŷ相同）：
     - 在关键点的等价位置 `˜p` 上，放置一个**高斯核 (Gaussian Kernel)**：$Y(x, y, c) = exp( - ( (x - ˜p_x)² + (y - ˜p_y)² ) / (2 * σ_p²) )$
     - **高斯核的作用：** 让目标点 `˜p` 的位置是峰值（值为1），随着距离增大，响应值按高斯分布衰减到0。这比用1/0的二值标签更平滑，更有利于训练收敛（梯度更好）。
     - **σ_p (自适应标准差)：** 这个高斯核的标准差 $σ_p$ **不是固定**的，而是**根据目标大小自适应变化**（通常基于包围框大小计算）。大的目标用较大的 $σ_p$（产生宽峰），小的目标用较小的 $σ_p$（产生尖峰），这样能更好地反映不同大小目标的关键点定位不确定性。
     - **同类别关键点重叠处理：** 如果属于**同一类别** `c` 的两个关键点的低分辨率位置 `˜p` 非常接近，以至于它们的高斯区域发生重叠，则直接在重叠区域的每个像素点上取最大值 (element-wise maximum)。

3. **损失函数 (Loss Function)：**
    训练网络 $Ŷ$ 的目标是让它预测的热图尽可能接近真值热图 `Y`。使用**带惩罚缩放的逐像素逻辑回归 + 焦点损失Focal Loss**。

   - 核心思想：
     - **逐像素逻辑回归：** 把预测每个像素点对应类别 `c` 的值看作是独立的二分类问题。$Ŷ(x,y,c)$ 经过 sigmoid 激活后就可以理解为概率值 $(sigmoid(Ŷ(x,y,c)))$。
     - **惩罚缩放：** 这里指对 `正样本 (Y=1)` 区域的损失权重进行特殊处理（降低惩罚），如损失公式所示。
     - **Focal Loss：** 用来解决**类别极度不平衡**（背景像素 `Y=0` 非常多，关键点像素 `Y=1` 非常少）带来的问题。它通过调制因子 $(1 - Ŷ)α$ 和 $(Y)β$ 自动衰减易分类样本（特别是大量易分的背景像素）对总损失的贡献，从而使模型训练时更加关注难分的样本。
   - **具体公式：**

   $L_k = - \frac{1}{N} \sum_{x,y,c}
   \begin{cases}
     (1 - Ŷ_{xyc})^\alpha \log(Ŷ_{xyc}) & \text{如果 } Y_{xyc} = 1 \quad \text{(正样本)} \\
     (1 - Y_{xyc})^\beta (Ŷ_{xyc})^\alpha \log(1 - Ŷ_{xyc}) & \text{否则 (负样本)}
   \end{cases}$

   - 解释：
     - `L_k`：关键点预测的损失值。
     - `N`：图像中**真实关键点的总数**。用来归一化正样本的损失贡献，使得所有正样本的总损失权重约为 1。
     - `x, y, c`：遍历输出特征图上的所有位置和所有类别。
     - **第一项 (Y=1)：** 对正样本（关键点区域），使用 $(1 - Ŷ)^α * log(Ŷ)$。$(1 - Ŷ)^α$ 就是Focal Loss的调制因子。当预测值 $Ŷ$ 接近1（预测完全正确）时，$(1 - Ŷ)^α$ 很小（α>0），该项损失就很小；当 `Ŷ` 接近0（预测错误）时，$(1 - Ŷ)^α$ 接近1，该项损失就是 $log(Ŷ)$ 的加权，因为$log(Ŷ)$在`Ŷ`接近0时为很大的负值，整个损失会比较大。`α` 控制了对预测良好的正样本的权重衰减程度（`α=2` 表示衰减较强）。
     - **第二项 (Y≠1)：**对负样本（背景），使用 $(1 - Y)^β * (Ŷ)^α * log(1 - Ŷ)$。(Ŷ)^α也是调制因子。当预测值 Ŷ接近0（预测正确是背景）时，(Ŷ)^α很小，该项损失衰减得很厉害；当 Ŷ接近1（预测错误把背景预测成了关键点）时，(Ŷ)^α 接近1，损失接近 $(1 - Y)^β * log(1 - Ŷ)$。
       - $(1 - Y)^β$: 是真值热图值（接近0但不正好是0，因为高斯分布）。这个因子降低了那些在真值高斯核**边缘区域**（值很小但不是0，`1-Y`很大）的负样本的损失权重（因为是自然模糊区），提高了**远离所有关键点纯背景区域**（`Y=0`，所以 $1-Y≈1$）的损失权重（对于预测为关键点的错误惩罚更大）。`β=4` 赋予了这种调节更强的效果。
     - $log(1 - Ŷ)$： 当负样本被预测为关键点时（`Ŷ`接近1），该值接近负无穷（损失很大）。
   - **超参数：** `α=2`, `β=4` 是实验确定的常用值。

4. **局部偏移预测 (Local Offset Prediction)：**

   - **目的：** 补偿**输出步长 `R` 带来的量化误差 (Discretization Error)**。当计算低分辨率等价位置 $˜p = floor(p / R)$ 时，丢弃了原始坐标 $p / R$ 的小数部分。这导致了定位精度损失，最大可达 `R-1` 个像素（当 `R=4`，最大误差3个像素）。
   - **输出：** 模型额外预测一张**局部偏移图 `Ô`**，维度为 $(W/R) × (H/R) × 2$。它的宽度和高度与关键点热图 `Ŷ` 相同。最后一个维度 `2` 表示 (x偏移, y偏移)。
   - **属性：** **所有类别 `c` 共享同一个偏移预测**。预测的是位置级别的偏移，而不是类别级别的偏移。
   - **训练损失：** 使用 `L1` 损失。$L_{off} = \frac{1}{N} \sum_p \left| Ô_{\tilde{p}} - \left( \frac{p}{R} - \tilde{p} \right) \right|$

   - 解释：
     - `L_off`：偏移预测损失。
     - `p`：遍历所有真实关键点。
     - $Ô_{\˜p}$：模型在低分辨率位置 `˜p` 预测的偏移向量。
     - $(p / R - ˜p)$：真实的偏移量。`p/R` 是真实关键点在低分辨率网格上的精确浮点位置，`˜p` 是其向下取整的整数位置。它们的差值 `(p/R - ˜p)` 就是需要预测的小数部分偏移（在 `[-1, 1)` 区间内）。
     - **监督范围：** **只在真实关键点的位置 `˜p` 上进行监督**。输出图中不是关键点位置的其他地方产生的偏移预测被忽略，不参与损失计算（因为这些地方没有真实关键点，也就没有“真实”的偏移）。

------

### 中心点回归（center point regression）

这段文字描述的是一种**基于中心点回归（center point regression）**的目标检测算法框架**，其核心思想是：通过神经网络预测每个目标的中心点、尺寸（宽高）、偏移量，从而恢复出目标边界框（bounding box），**无需使用传统的IoU或非极大值抑制（NMS）方法。

------

1. **基本概念与符号说明**

**(1) 边界框表示与中心点**

对于第 k 个物体，其边界框为：$(x1(k),y1(k),x2(k),y2(k))(x^{(k)}_1, y^{(k)}_1, x^{(k)}_2, y^{(k)}_2)$

- $x1(k),y1(k)x^{(k)}_1, y^{(k)}_1$ 是左上角坐标；
- $x2(k),y2(k)x^{(k)}_2, y^{(k)}_2$ 是右下角坐标。

中心点：

$pk=(x1(k)+x2(k)2,y1(k)+y2(k)2)p_k = \left( \frac{x^{(k)}_1 + x^{(k)}_2}{2}, \frac{y^{(k)}_1 + y^{(k)}_2}{2} \right)$

物体尺寸：

$sk=(x2(k)−x1(k),y2(k)−y1(k))s_k = \left( x^{(k)}_2 - x^{(k)}_1, y^{(k)}_2 - y^{(k)}_1 \right)$

(2) 网络预测

网络预测三个关键变量：

- $\hat{Y}$：预测的热力图（heatmap），代表每一类的中心点概率；
- $O^∈RWR×HR×2\hat{O} \in \mathbb{R}^{\frac{W}{R} \times \frac{H}{R} \times 2}$：中心点偏移量（offset）；
- $S^∈RWR×HR×2\hat{S} \in \mathbb{R}^{\frac{W}{R} \times \frac{H}{R} \times 2}$：中心点处的物体尺寸（size：宽高）。

其中 R 是下采样率（stride），如输出热力图大小是输入图像的 1/R1/R。

------

2. **损失函数设计**

(1) 物体尺寸损失$ L{\text{size}}$

对于每个目标中心点位置 $p_k$，网络输出的尺寸预测为 $\hat{S}_{p_k}$，真实尺寸为 $s_k$。

损失函数：

$Lsiz= \frac{1}{N} \sum_{k=1}^N \left\| \hat{S}_{p_k} - s_k \right\|$

- 使用 **L1 损失**；
- 直接使用原始像素坐标，不进行归一化；
- 损失乘以常数系数 $\lambda_{\text{size}} = 0.1$ 用于平衡整体损失。

(2) 总体损失函数

整个检测损失包含三项：

$L_{\text{det}} = L_k + \lambda_{\text{size}} L_{\text{size}} + \lambda_{\text{off}} L_{\text{off}}$

- $L_k$：中心点 heatmap 的损失（例如 focal loss）；
- $LoffL_{\text{off}}$：偏移损失（offset），解决中心点位置落在非整数像素点的误差；
- 默认设置：$\lambda_{\text{size}} = 0.1, \lambda_{\text{off}} = 1$

------

3. **网络结构与多任务学习**

- 该检测器采用 **共享主干网络（backbone）**（如 ResNet 或 HRNet）；
- 每个任务（heatmap、offset、size）各有单独的分支：
  - **3×3卷积 + ReLU + 1×1卷积**；
- 输出维度为 C+4C + 4（C类热力图 + 2维偏移 + 2维尺寸）；
- 所有分支共享特征，但独立学习任务。

------

4. **推理过程（Inference）**

(1) 从热力图中提取中心点

- 对每一类的热力图独立处理；
- 用 **3×3 最大池化（max pooling）**找出局部极大值（即峰值），作为潜在中心点；
- 保留每类热力图中得分最高的 100 个峰值；

(2) 通过偏移和尺寸恢复边界框

对于检测到的第 i 个中心点：

- 坐标为整数：$(x^i,y^i)(\hat{x}_i, \hat{y}_i)$
- 预测偏移：
- 预测尺寸：

最终边界框计算公式为：

$\left( \hat{x}_i + \delta \hat{x}_i - \frac{\hat{w}_i}{2}, \hat{y}_i + \delta \hat{y}_i - \frac{\hat{h}_i}{2}, \hat{x}_i + \delta \hat{x}_i + \frac{\hat{w}_i}{2}, \hat{y}_i + \delta \hat{y}_i + \frac{\hat{h}_i}{2} \right)$

这种方式直接从中心点回归出目标的完整边界框，**不需要使用基于 IoU 的 NMS 后处理操作**。

------

### 3D detection

3D检测估计每个对象的三维边界框，并需要每个中心点的三个附加属性：**深度，3D尺寸和方向**。我们为它们中的每一个添加单独的头部。

深度d是每个中心点的单个标量。然而，深度很难直接回归。直接回归深度 d（通常范围在 [1,100]m）可能导致训练不稳定，尤其是**远处物体深度变化小但误差影响大**。因此，采用了以下变换方法来让模型在输出层预测一个“变换后的深度” d^∈[0,1]，再通过**逆变换**恢复实际深度。

**网络输出的是：**$\hat{d} \in [0, 1]$

**真实深度计算方式为：**$d = \frac{1}{\sigma(\hat{d})} - 1$。其中$ σ(x)= \frac{1}{1 + e^{-x}}σ(x)$ 是 Sigmoid 函数。

**原理解释：**

- 该变换确保输出范围处于 [0,+∞)，适合建模深度这种正实数；
- 由于 sigmoid 函数的反函数在小值和大值区间都趋于饱和（梯度变小），在某种程度上更关注较近目标；
- 网络只需输出一个稳定的归一化深度变量$\hat{d} \in [0,1]$，训练更容易收敛。

物体的三维尺寸是三个标量，我们使用一个单独的分离头$\Gamma \in \mathbb{R}^{\frac{W}{R} \times \frac{H}{R} \times 3}$和一个L1损失直接回归到它们的绝对值（以米为单位）。


默认情况下，方向是单个标量。然而，它可能很难回归。我们遵循Mousavian等人[38]，并将方向表示为两个bin，并使用bin回归。


具体而言，方向使用8个标量编码，每个bin使用4个标量。对于一个bin，两个标量用于softmax分类，其余两个标量回归到每个bin内的角度。

------

## CFTrack: Center-based Radar and Camera Fusion for 3D Multi-Object Tracking

### 引言

3D多目标跟踪是自动驾驶车辆感知系统的重要组成部分。跟踪车辆周围的所有动态目标对于避障和路径规划等任务至关重要。自动驾驶车辆通常配备不同的传感器模态以提高准确性和可靠性。近年来，传感器融合已广泛应用于目标检测网络，大多数现有的多目标跟踪算法或者依赖于单个输入模态，或者没有充分利用由多个感测模态提供的信息。在这项工作中，提出了一种基于雷达和摄像机传感器融合的端到端目标检测与跟踪网络。我们提出的方法使用基于中心的雷达-相机融合算法用于对象检测，并**利用贪婪算法进行对象关联**。所提出的贪婪算法使用被检测物体的**深度，速度和2D位移**来将它们与时间相关联。这使得我们的跟踪算法对遮挡和重叠的物体非常鲁棒，因为深度和速度信息可以帮助网络区分它们。

多目标跟踪（MOT）是分析视频以识别和跟踪属于特定类别的物体的任务，而无需事先了解目标的外观或数量[1]。具有相似外观的物体之间的遮挡和相互作用是使MOT成为一项具有挑战性的任务的两个主要因素。近年来，已经开发了许多算法来解决这些问题。这些算法大多利用深度神经网络（DNN）的丰富表示能力从输入中提取复杂的语义特征。通过检测进行跟踪是这些算法中使用的一种常见方法，其中跟踪问题通过将其分为两个步骤来解决：（1）检测每个图像中的对象，（2）随着时间的推移将检测到的对象关联起来。

我们提出了一种**端到端的MOT框架**，利用雷达和摄像机数据进行联合目标检测和跟踪。我们的方法基于CenterFusion[7]，这是一种用于自动驾驶应用中3D物体检测的雷达-相机传感器融合算法。CenterFusion使用雷达和相机融合在3D对象检测方面实现了最先进的性能，并提供了对对象跟踪任务非常有帮助的对象速度估计。我们提出的网络除了前一帧和检测到的物体外，还将当前图像帧和雷达检测作为输入。输出是3D对象检测结果和检测到的对象的跟踪ID。每个检测到的物体还与全局坐标系中的估计绝对速度相关联。

我们跟踪框架中的对象关联步骤基于类似于CenterTrack[3]的简单贪婪算法。虽然CenterTrack仅使用连续图像中的对象的2D位移将它们关联起来，但我们提出了一种**基于加权成本函数的贪婪算法**，该函数是根据对象的估计深度和速度以及其2D位移计算的。这显著提高了网络正确关联被遮挡和重叠对象的能力，因为深度和速度信息为区分这些对象提供了有价值的线索。此外，提出的网络使用融合的雷达和图像特征来预测对象在连续帧中的位移，这使得这些预测与仅使用视觉信息相比更加准确。

MOT算法可以分为**批处理和在线方法**。批处理方法**使用整个帧序列来找到检测之间的全局最佳关联**。这类方法中的大多数方法都基于光流算法，并从整个序列创建流图[18]，[19]。另一方面，在线方法**仅使用当前帧的信息来跟踪对象**。这些算法中的许多算法都会产生二分图匹配问题[20]，该问题使用匈牙利算法解决。这一类中的更多现代方法使用深度神经网络来解决关联问题[21]，[22]。我们提出的算法是一种在线方法，不需要来自未来帧的任何知识。

------

### 模型架构

#### CenterFusion

具体细节见《CenterFusion: Center-based Radar and Camera Fusion for 3D Object Detection》

![image-20250712152606628](C:\Users\adminqiu\AppData\Roaming\Typora\typora-user-images\image-20250712152606628.png)

网络的输入显示在左侧，其中包括当前图像和雷达点云（顶部）、之前的图像帧和雷达点云（中间）以及以类别不可知热图形式呈现的之前检测结果（底部）。雷达点云显示在输入图像上。模型中添加了一个**额外的回归头（“Dis”）**，该模型使用融合的雷达和图像特征来预测连续帧中的对象位移。关联步骤中的贪婪算法使用每个对象的位移、深度和速度将其与之前的检测相关联。输出是所有检测到的对象的3D边界框和轨迹ID。

在 CenterFusion 网络的基础上进行了修改。除了**当前帧图像** $I^{(t)}$ 和**当前帧雷达点迹** $P^{(t)}$ 之外，修改后的网络还以**上一帧图像** $I^{(t-1)}$、**上一帧雷达点迹** $P^{(t-1)}$ 以及**已检测到的目标对象**（来自上一帧）作为输入。

网络的输出是：

1. 所有检测到目标对象的 **3D 边界框**
2. 每个目标在车辆坐标系下沿 **x 和 y 方向**的**绝对速度** $\mathbf{v} = (v_x, v_y)$。

**如何表示上一帧检测结果：**
 上一帧检测到的目标被表示为一个​**​单通道热力图​**​，这是使用​**​二维高斯核 (2D Gaussian kernel)​**​ 生成的。

**融合多帧/多传感器信息的意义：**

- 融合上一帧的图像 $I^{(t-1)}$、雷达点迹 $P^{(t-1)}$ 以及检测结果，有助于网络**更准确地估计**目标在当前帧 $I^{(t)}$ 中的位置。
- 来自上一帧 $P^{(t-1)}$ 的雷达信息，进一步**增强了网络在发生遮挡导致缺乏视觉证据情况下的目标检测能力**。

除了当前帧的检测结果，改进的网络还利用​**​拼接后的雷达与图像特征 (concatenated radar and image features)​**​，​**​**估计​​已检测目标在​​当前帧与​​上一帧​​之间的​​二维位移 (2D displacement)​​。

**融合信息提升位移预测精度：**

- 结合**当前帧和上一帧的图像特征**
- 融合**雷达提供的深度 `d` 和速度 $\mathbf{v}$ 信息**

------

#### CenterTrack

我们遵循CenterTrack [3]并从局部角度处理跟踪问题，其中**对象的身份会在连续帧中保留，而不会在对象离开帧时重新建立关联**。我们同时使用前一帧的摄像机和雷达数据来提高跟踪当前帧中被遮挡物体的能力。CFTrack利用融合的雷达和图像特征来估计连续帧中目标的位移，用于通过时间进行物体关联。在关联步骤中，提出了一种贪婪算法，该算法除了利用物体的二维位移外，还利用物体的速度和深度信息进行时间上的精确关联。

CFTrack 的输入包括：

- 当前帧和上一帧图像：$I^{(t-1)}, I^{(t)} \in \mathbb{R}^{W\times H\times 3}$
- 当前帧和上一帧雷达检测点迹：$P^{(t-1)}, P^{(t)} \in \mathbb{R}^{N\times 5}$，其中 `N` 为雷达点迹数量
- 上一帧的已追踪目标列表：$T^{(t-1)} = \{b^{(t-1)}_0, b^{(t-1)}_1, \dots\}$

追踪目标对象 `b` 的表示形式为： $b = (p, d, v, w, id)$
 其中：

- $p \in \mathbb{R}^2$ 表示目标中心的（图像）坐标位置
- $d \in \mathbb{R}$ 表示目标深度
- $v \in \mathbb{R}^2$ 表示目标速度
- $w \in [0, 1]$ 表示检测置信度
- `id` 是表示该追踪目标唯一身份的整型标识符。

在每一帧中，算法需要完成的目标是检测并追踪目标对象 $T^{(t)} = \{b^{(t)}_0, b^{(t)}_1, \dots\}$，并在连续帧中为目标对象赋予一致的 `id`。目标检测与关联由一个端到端训练的单深度网络完成。

------

##### **目标关联算法**

![image-20250712160145067](C:\Users\adminqiu\AppData\Roaming\Typora\typora-user-images\image-20250712160145067.png)

顶部：对象与前一帧的位移，由从当前帧中每个对象的中心指向前一帧中同一对象的估计中心的箭头表示。底部：上一帧。重新绘制了位移箭头以进行比较。

------

我们采用一种**贪心算法**来关联跨时间帧的检测目标。
 检测到的目标对象表示为 $a = (p,\; d,\; v,\; c)$，其中：

- $p \in \mathbb{Z}^2$ 表示目标中心在图像上的**像素坐标**
- $d \in \mathbb{R}$ 表示目标**深度**
- $v \in \mathbb{R}^2$ 表示目标**速度** (在车辆坐标系中)
- $c \in \mathcal{C}$ 表示目标**类别**

与文献[3]类似，**目标在图像上的位移**是通过网络中的一个**回归层**计算的。该回归层以**两个输出通道**的形式输出位移信息：$\widehat{D}^{(t)} \in \mathbb{R}^{\frac{W}{R} \times \frac{H}{R} \times 2}$，表示目标中心的二维图像位移（如图 2 所示）。与其他回归头（regression heads）类似，该层使用 **$L_1$ 损失函数**作为训练目标。

为了跨帧关联目标对象，我们定义了一个基于目标**深度**、**速度**和**图像平面位移**的**成本函数** $\text{Cost}_{t, t-1}$：

$\text{Cost}_{t,t-1} = \begin{cases}
\alpha \cdot L_{\text{pixel}} + \beta \cdot L_{\text{depth}} + \delta \cdot L_{\text{velocity}} & \text{若 } c_t = c_{t-1} \\
1 & \text{若 } c_t \neq c_{t-1}
\end{cases}$

其中：

$L_{\text{pixel}} &= (x_t - x_{t-1})^2 + (y_t - y_{t-1})^2 \\
L_{\text{depth}} &= (d_t - d_{t-1})^2 \\
L_{\text{velocity}} &= (v_{x_t} - v_{x_{t-1}})^2 + (v_{y_t} - v_{y_{t-1}})^2\\$

*   $(x, y)$ 表示目标中心的像素坐标
*   $d$ 表示目标深度
*   $v_x$, $v_y$ 分别表示目标在车辆坐标系下 x 和 y 方向的速度分量。
*   $\alpha, \beta, \delta \in \mathbb{R}^+$ 是**可调参数** ($\mathbb{R}^+$ 表示正实数集合)。

**关联匹配过程：**
对于当前帧 $t$ 在位置 $p$ 检测到的每一个目标对象：

1.  **预测搜索位置：** 在**上一帧 $t-1$** 的检测中，在位置 $p - \widehat{D}_p$ (即当前帧位置减去网络预测的位移 $\widehat{D}_p$) **周围**的一个**半径 $r$** 范围内，寻找可能匹配的检测目标。
2.  **计算成本并匹配：** 如果在步骤 1 的位置附近存在**尚未匹配**的上一帧检测目标，则计算它们之间的成本函数 $\text{Cost}_{t, t-1}$，以确定这些检测之间的“距离”（差异）。**当前帧目标将与上一帧所有候选目标中成本最低者进行匹配**。
3.  **新建轨迹：** 对于当前帧中**最终未能与任何上一帧目标匹配**的检测目标，将为其**创建一个新的轨迹（新 ID）**。
